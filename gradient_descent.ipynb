{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xet ham so f(x) = x ^ 2 + 5sin(x) voi dao ham f'(x) = 2x + 5cos(x). Quy tac gradient descent x_(t + 1) = x_t - n(2x_t + 5cos(x_t))\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5, -4.141831092731613, -3.0434140487394945, -1.9371390635788721, -1.370609623535342, -1.1959138533062952, -1.1398126662660861, -1.1207324901805855, -1.1140974995041208, -1.1117718342401366, -1.1109543623859697, -1.1106667365268623] 11\n",
      "[5, 3.8581689072683867, 3.463564567930569, 3.2451582916682646, 3.0934475688734215, 2.9741786797296776, 2.8723524342019475, 2.7798685851337033, 2.691538912182054, 2.6034429924417726, 2.512083663118539, 2.413825273788166, 2.3043909242955314, 2.178284700900974, 2.028031263811057, 1.8431593967550366, 1.6090315913519224, 1.3063382475764564, 0.9143774850440367, 0.42636006838025575, -0.11415049832376245, -0.5880663503407273, -0.8864605464168874, -1.0252476809653677, -1.079641732011138, -1.0995355411928174, -1.1066334337506414, -1.1091439570842945, -1.1100292207856688, -1.1103410483948122] 29\n"
     ]
    }
   ],
   "source": [
    "# Tinh dao ham f'(x)\n",
    "def grad(x):\n",
    "    return 2 * x + 5 * np.cos(x)\n",
    "\n",
    "# Tinh f(x)\n",
    "def cost(x):\n",
    "    return x ** 2 + 5 * np.sin(x)\n",
    "\n",
    "def myGD1(x0, theta):\n",
    "    x = [x0]\n",
    "    for it in range(1000):\n",
    "        x_new = x[-1] - theta * grad(x[-1])\n",
    "        if abs(grad(x_new)) < 1e-3:\n",
    "            break\n",
    "        x.append(x_new)\n",
    "    return (x, it)\n",
    "\n",
    "(x1, it1) = myGD1(-5, .1)\n",
    "(x2, it2) = myGD1(5, .1)\n",
    "print(x1, it1)\n",
    "print(x2, it2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.007522942065274 4.245681353449078\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent nhieu bien \n",
    "# Tao 1000 diem du lieu quanh duong thang y = 4 + 3x\n",
    "# Su dung scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X = np.random.rand(1000, 1)\n",
    "y = 4 + 3 * X + .5 * np.random.rand(1000, 1)\n",
    "model = LinearRegression()\n",
    "model.fit(X.reshape(-1, 1), y.reshape(-1, 1))\n",
    "\n",
    "(w, b) = model.coef_[0][0], model.intercept_[0]\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.2599633 ]\n",
      " [2.98063668]] 46\n"
     ]
    }
   ],
   "source": [
    "# Su dung gradient descent cho ham mat mat L(w) = 1 / 2N * l2_norm(y - X.T * w) ^ 2, grad_w of L(w) is 1 / N * X * (X.T * w - y)\n",
    "\n",
    "one = np.ones((X.shape[0], 1))\n",
    "Xbar = np.concatenate((one, X.reshape(-1, 1)), axis=1)\n",
    "\n",
    "def grad2(w):\n",
    "    N = Xbar.shape[0]\n",
    "    return 1 / N * Xbar.T.dot(Xbar.dot(w) - y)\n",
    "\n",
    "def cost2(w):\n",
    "    N = Xbar.shape[0]\n",
    "    return .5 / N * np.linalg.norm(y - Xbar.dot(w)) ** 2\n",
    "\n",
    "def myGD(w_init, theta):\n",
    "    w = [w_init]\n",
    "    for it in range(100):\n",
    "        w_new = w[-1] - theta * grad2(w[-1])\n",
    "        if np.linalg.norm(grad2(w_new)) / len(w_new) < 1e-3:\n",
    "            break\n",
    "        w.append(w_new)\n",
    "    return (w, it)\n",
    "\n",
    "w_init = [[2], [1]]\n",
    "w1, it1 = myGD(w_init, 1)\n",
    "print(w1[-1], it1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.230418  ]\n",
      " [2.99448792]] 97\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent with momentum for f(x) = x ^ 2 + 10sin(x)\n",
    "def GD_momentum(theta_init, eta, gamma):\n",
    "    #Suppose we want to store history of theta\n",
    "    theta = [theta_init]\n",
    "    v_old = np.zeros_like(theta_init)\n",
    "    for it in range(100):\n",
    "        v_new = gamma * v_old + eta * grad2(theta[-1])\n",
    "        theta_new = theta[-1] - v_new\n",
    "        if np.linalg.norm(grad2(theta_new)) / np.array(theta_init).size < 1e-3:\n",
    "            break\n",
    "        v_old = v_new\n",
    "        theta.append(theta_new)\n",
    "    return (theta, it)\n",
    "\n",
    "w, it = GD_momentum([[2], [1]], 1, .9)\n",
    "print(w[-1], it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.22143242]\n",
      " [3.05317148]] 26\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent with momentum using NAG\n",
    "def GD_NAG(theta_init, eta, gamma):\n",
    "    theta = [theta_init]\n",
    "    v = [np.zeros_like(theta_init)]\n",
    "    for it in range(100):\n",
    "        v_new = gamma * v[-1] + eta * grad2(theta[-1] - gamma * v[-1])\n",
    "        theta_new = theta[-1] - v_new\n",
    "        if np.linalg.norm(grad2(theta_new)) / np.array(theta_init).size < 1e-3:\n",
    "            break\n",
    "        theta.append(theta_new)\n",
    "        v.append(v_new)\n",
    "    return (theta, it)\n",
    "\n",
    "w, it = GD_NAG([[2], [1]], .5, .9)\n",
    "print(w[-1], it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.26316537]\n",
      " [3.02729627]] 220\n"
     ]
    }
   ],
   "source": [
    "#SGD\n",
    "def grad_SGD(xi, yi, w):\n",
    "    xi = xi.reshape(1, -1)\n",
    "    return xi.T.dot(xi.dot(w) - yi)\n",
    "\n",
    "def SGD(theta_init, eta):\n",
    "    it = 0\n",
    "    theta = [theta_init]\n",
    "    while it < 100:\n",
    "        id = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "        kt = 0\n",
    "        for k in id:\n",
    "            it += 1\n",
    "            theta_new = theta[-1] - eta * grad_SGD(Xbar[k], y[k], theta[-1])\n",
    "            if np.linalg.norm(grad_SGD(Xbar[k], y[k], theta[-1])) / np.array(theta_new).size < 1e-3:\n",
    "                kt = 1\n",
    "                break\n",
    "            theta.append(theta_new)\n",
    "        if kt == 1:\n",
    "            break\n",
    "    return (theta, it)\n",
    "\n",
    "w, it = SGD([[2], [1]], .5)\n",
    "print(w[-1], it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "t = [1, 2, 3]\n",
    "print(np.zeros_like(t))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
