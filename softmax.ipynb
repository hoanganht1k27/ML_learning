{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.51175343 0.41898827 0.0692583 ]\n",
      " [0.50648039 0.30719589 0.18632372]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(Z):\n",
    "    # Compute softmax values for each sets of scores in Z each column of Z is a set of scores.\n",
    "    # Z: a numpy array of shape (N, C)\n",
    "    # return a numpy array of shape (N, C)\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "print(softmax(np.array([[2, 2, 2], [2, 1.8, 0], [-1, -1.5, -2]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.51175343 0.41898827 0.0692583 ]\n",
      " [0.50648039 0.30719589 0.18632372]]\n"
     ]
    }
   ],
   "source": [
    "# phien ban on dinh hon cua ham softmax (chong tran so)\n",
    "def softmax_stable(Z):\n",
    "    # Compute softmax values for each set of scores in Z each row of Z is a set of scores.\n",
    "    # Z is N * C\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "print(softmax_stable(np.array([[2, 2, 2], [2, 1.8, 0], [-1, -1.5, -2]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  7 11]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "b = a.sum(axis=1, keepdims=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(X, y, W):\n",
    "    # W: 2d numpy array of shape (d, C), each column correspoding to one output node\n",
    "    # X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    # y: 1d numpy array -- label of each row of X\n",
    "\n",
    "    A = softmax_stable(X.dot(W))\n",
    "    id0 = range(X.shape[0]) # range(N), indexes in axis 0 is id0, indexes in axis 1 is y\n",
    "    return -np.mean(np.log(A[id0, y]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_grad(X, y, W):\n",
    "    # W: 2d numpy array of shape (d, C), each column correspoding to one output node\n",
    "    # X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    # y: 1d numpy array -- label of each row of X\n",
    "    A = softmax_stable(X.dot(W))\n",
    "    id0 = range(X.shape[0])\n",
    "    A[id0, y] -= 1\n",
    "    return X.T.dot(A) / X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fit(X, y, W, lr = 0.01, nepoches = 100, tol = 1e-5, batch_size = 10):\n",
    "    W_old = W.copy()\n",
    "    ep = 0\n",
    "    loss_hist = [softmax_loss(X, y, W)] # store loss history\n",
    "    N = X.shape[0]\n",
    "    nbatches = int(np.ceil(float(N) / batch_size))\n",
    "    while ep < nepoches:\n",
    "        ep += 1\n",
    "        mix_ids = np.random.permutation(N)\n",
    "        for i in range(nbatches):\n",
    "            # get the i-th batch\n",
    "            batch_ids = mix_ids[batch_size * i : min(batch_size * (i + 1), N)]\n",
    "            X_batch = X[batch_ids]\n",
    "            y_batch = y[batch_ids]\n",
    "            W = -lr * softmax_grad(X_batch, y_batch, W) # gradient descent\n",
    "            loss_hist.append(softmax_loss(X, y, W))\n",
    "            if np.linalg.norm(W - W_old) / W.size < tol:\n",
    "                print(ep)\n",
    "                break\n",
    "            W_old = W.copy()\n",
    "    return W, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(W, X):\n",
    "    return np.argmax(X.dot(W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[10, 2, 3], [4, 5, 6]])\n",
    "print(np.argmax(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi du bang data \n",
    "C, N = 5, 500\n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick\n",
    "\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N) # label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "W_init = np.random.randn(Xbar.shape[1], C)\n",
    "W, loss_hist = softmax_fit(Xbar, y, W_init, lr = 0.01)\n",
    "\n",
    "print(pred(W, np.array([[14.2, 2.1, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C = 1e5, solver='lbfgs', multi_class='multinomial')\n",
    "model.fit(X, y)\n",
    "\n",
    "X_test = np.array([[14.2, 2.1], [3.1, 6]])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05729193 -0.22016065  0.55028735 -1.06914942  0.79631464]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[14.2, 2.1, 1], [3.1, 6, 1]])\n",
    "# print(pred(W, Xbar[0:100]))\n",
    "print(Xbar[0:1].dot(W))\n",
    "# print(Xbar[0:10])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
