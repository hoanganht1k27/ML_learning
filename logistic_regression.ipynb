{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression with weight decay\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(S):\n",
    "    # S is a numpy array\n",
    "    return 1 / (1 + np.exp(-S))\n",
    "\n",
    "# f(x_i.T * w) or p(y_i | x_i; w)\n",
    "def prob(w, X):\n",
    "    # X: N * d\n",
    "    # w: d\n",
    "    return sigmoid(X.dot(w))\n",
    "\n",
    "def loss(w, X, y, lam):\n",
    "    # X, w as in prob\n",
    "    # y: N, each = 0 or 1\n",
    "    a = prob(w, X)\n",
    "    loss_0 = -np.mean(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "    weigh_decay = 0.5 * lam / X.shape[0] * np.sum(w * w)\n",
    "    return loss_0 + weigh_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.54337021 -4.06486702]\n",
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(w_init, X, y, lam, lr = 0.1, nepoches = 2000):\n",
    "    # lam: regularization parameter\n",
    "    # lr: learning rate\n",
    "    # nepoches: number of epoches\n",
    "    N, d = X.shape[0], X.shape[1]\n",
    "    w = w_old = w_init\n",
    "    # store history of loss in loss_hist\n",
    "    loss_hist = [loss(w_init, X, y, lam)]\n",
    "    ep = 0\n",
    "    while ep < nepoches:\n",
    "        ep += 1\n",
    "        mix_ids = np.random.permutation(N)\n",
    "        for i in mix_ids:\n",
    "            xi = X[i]\n",
    "            yi = y[i]\n",
    "            ai = sigmoid(xi.dot(w))\n",
    "            # update\n",
    "            w = w - lr * ((ai - yi) * xi + lam * w)\n",
    "            if np.linalg.norm(w - w_old) / d < 1e-6:\n",
    "                break\n",
    "            w_old = w\n",
    "    return w, loss_hist\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "X = np.array([[0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50]]).T\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
    "w_init = np.random.randn(Xbar.shape[1])\n",
    "lam = 0.0001\n",
    "w, loss_hist = logistic_regression(w_init, Xbar, y, lam, lr=0.05, nepoches=500)\n",
    "print(w)\n",
    "\n",
    "def predict(w, X, threshold = 0.5):\n",
    "    # predict output for each row of X\n",
    "    # X: N * d\n",
    "    res = np.zeros(X.shape[0])\n",
    "    res[np.where(prob(w, X) > threshold)[0]] = 1\n",
    "    return res\n",
    "\n",
    "print(predict(w, np.array([[0.5, 1], [2, 1], [5.5, 1]])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
